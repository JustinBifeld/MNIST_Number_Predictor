{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Number_Predictor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXJGCrB4dBfBy+ES78IaGU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justin02-dev/MNIST_Number_Predictor/blob/master/MNIST_Number_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9yGhyhL1Hhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install necessary libraries\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTB1WQ0p7wz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import necessary libraries\n",
        "import torch #overall torch package\n",
        "import torch.nn as nn #subpackage to help with building networks\n",
        "import torch.optim as optim #subpackage with optimizers like SGD or Adam\n",
        "import torch.nn.functional as F #Interface with typical operations for building networks like loss and convolutions\n",
        "\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader #dataloader for iterating through data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vshhf-Ll8VIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load MNIST training dataset, store in /data, convert to a Tensor, Normalized\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root='./data'\n",
        "    ,train=True\n",
        "    ,download=True\n",
        "    ,transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "    ])\n",
        ")\n",
        "#load the data with a batch size of 1000 and shuffle it\n",
        "train_loader = torch.utils.data.DataLoader(train_set\n",
        "    ,batch_size=1000\n",
        "    ,shuffle=True\n",
        ")\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2MxJ6VZi3rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define hyperparameters\n",
        "batch_size = 100\n",
        "lr = 0.01\n",
        "\n",
        "#create our Netowrk\n",
        "network = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "    , nn.ReLU()\n",
        "    , nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    , nn.BatchNorm2d(6)\n",
        "    , nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "    , nn.ReLU()\n",
        "    , nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    , nn.Flatten(start_dim=1)  \n",
        "    , nn.Linear(in_features=12*4*4, out_features=120)\n",
        "    , nn.ReLU()\n",
        "    , nn.BatchNorm1d(120)\n",
        "    , nn.Linear(in_features=120, out_features=60)\n",
        "    , nn.ReLU()\n",
        "    , nn.Linear(in_features=60, out_features=10)\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size) #load training data with a batch size\n",
        "optimizer = optim.Adam(network.parameters(), lr=lr) #create optimizer to use "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JjZltOEAPbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to return number of correct predictions\n",
        "def get_num_correct(preds, labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
        "\n",
        "#function to display image(s)\n",
        "def show_batch(batch):\n",
        "    im = torchvision.utils.make_grid(batch)\n",
        "    plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))  "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHhpHLBs94Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(10):\n",
        "  total_correct = 0\n",
        "  for data in train_loader:\n",
        "    images, labels = data\n",
        "    preds = network(images)\n",
        "    loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # Calculate Gradients\n",
        "    optimizer.step() # Update Weights\n",
        "    total_correct += get_num_correct(preds, labels)\n",
        "  #output data\n",
        "  print(\n",
        "      \"epoch:\", epoch, \n",
        "      \"total_correct:\", total_correct, \n",
        "      \"loss:\", loss.item(),\n",
        "      '% correct', total_correct/60000\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoCMRfF-lQ4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load MNIST testing dataset, store in /data, convert to a Tensor, Normalized\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root='./data'\n",
        "    ,train=False\n",
        "    ,download=True\n",
        "    ,transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "    ])\n",
        ")\n",
        "#load the data with a batch size of 1000 and shuffle it\n",
        "test_loader = torch.utils.data.DataLoader(test_set\n",
        "    ,batch_size=100\n",
        "    ,shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(test_set, batch_size=10000, shuffle=True, num_workers=1)\n",
        "data = next(iter(test_loader))\n",
        "print(data[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdDJtp9Dl2Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_correct = 0\n",
        "for data in test_loader:\n",
        "    images, labels = data\n",
        "    preds = network(images)\n",
        "    total_correct += get_num_correct(preds, labels)\n",
        "\n",
        "#output data\n",
        "print(\n",
        "    \"epoch:\", epoch, \n",
        "    \"total_correct:\", total_correct, \n",
        "    \"loss:\", loss.item(),\n",
        "    '% correct', total_correct/10000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_rkb7WtB52j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "992f4f5d-4201-4516-8733-b0c975562aff"
      },
      "source": [
        "index = 0 #display and classify the image at this index, 0-9999\n",
        "network.eval() \n",
        "total_correct = 0\n",
        "images, labels = data\n",
        "images = images[index]\n",
        "images = torch.unsqueeze(images, dim=0)\n",
        "labels = labels[index]\n",
        "preds = network(images)\n",
        "total_correct += get_num_correct(preds, labels)\n",
        "show_batch(images)\n",
        "#output data\n",
        "print( \n",
        "    \"guess:\", preds.argmax().item(),\n",
        "    \"label:\", labels,\n",
        "    \"correct:\", (total_correct==1)\n",
        ")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "guess: 2 label: tensor(2) correct: True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}